{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pump It Up: Data Mining the Water Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using data from Taarifa and the Tanzanian Ministry of Water, can you predict which pumps are functional, which need some repairs, and which don't work at all? This is an intermediate-level practice competition. Predict one of these three classes based on a number of variables about what kind of pump is operating, when it was installed, and how it is managed. A smart understanding of which waterpoints will fail can improve maintenance operations and ensure that clean, potable water is available to communities across Tanzania."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TL:DR: The aim of this assignment if to create a classification model out of the data provided. There are 3 labels ('functional', 'non functional', and 'functional needing repair'). As such we will not make use of logistic regression (even if using one vs rest aproach could be used, using tree classifiers will be simpler) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load our training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Training Set values.csv', header = 0)\n",
    "predictions = pd.read_csv('Training Set labels.csv', header = 0)\n",
    "predictions = predictions.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find out which columns have NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns[df.isnull().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funder refers to who funded the water well, as such NA will be renamed to 'unknown'.\n",
    "\n",
    "Installer refers to who installed the water well, as such NA will be renamed to 'unknown'.\n",
    "\n",
    "Subvillage refers to the smallest administrative unit in Tanzania (up to 23k people), we will use 'unknown' for the time being.\n",
    "\n",
    "Public_meeting unclear reference but is a boolean variable, NAs wil be replaced to False.\n",
    "Scheme_management' refers to the who operates the well, as such NAs renamed to 'Unknown'.\n",
    "\n",
    "Scheme_name  refers to the who operates the well, as such NAs renamed to 'Unknown'.\n",
    "\n",
    "Permit refers to wheter the waterpoint is permitted, since its boolean NAs will be replaced to False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing the missing values with 'Unknowns' as explained previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['funder'] =df.funder.fillna(\"Unknown\")\n",
    "df['installer'] =df.installer.fillna(\"Unknown\")\n",
    "df['subvillage'] =df.subvillage.fillna(\"Unknown\")\n",
    "df['scheme_management'] =df.scheme_management.fillna(\"Unknown\")\n",
    "df['scheme_name'] =df.scheme_name.fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing the missing values with FALSE as explained previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['public_meeting'] =df.public_meeting.fillna(False)\n",
    "df['permit'] =df.permit.fillna(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping columns\n",
    "We believe that there are duplicate variables in the dataset. To test this hypothesis we will compare the columns affected. We will proceed to delete the one deemed redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) We count the number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.id.count() # There are 59400 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) We compare those columns we believe are identical, if this is the case the sum will be 59400 (each TRUE = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum(df['quantity'] == df['quantity_group']) # identical\n",
    "sum(df['extraction_type'] == df['extraction_type_group']) # 96% idential **** However it seems its the same stuff but with different words (windmill = wind powered)\n",
    "sum(df['extraction_type'] == df['extraction_type_class']) # 64% identical it seems its a simplified version of the previous one and we will keep this one.\n",
    "sum(df['payment'] == df['payment_type']) # 64% identical **** However it seems its the same stuff but with different words (on failure = when payment fails), once changed identical\n",
    "sum(df['water_quality'] == df['quality_group']) # 15% identical **** However it seems its the same stuff but with different words (soft = good), once changed identical\n",
    "sum(df['source'] == df['source_type']) # 64% identical ****  However it seems its the same stuff but with different words (soft = good), once changed identical\n",
    "sum(df['quantity'] == df['quantity_group']) # 64% identical ****  However it seems its the same stuff but with different words (machine_dbh = borehole), once changed identical\n",
    "sum(df['waterpoint_type'] == df['waterpoint_type_group']) #  95%identical, we pick the simplest one aka group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Time to remove the columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop('source', axis=1)\n",
    "df = df.drop('source_type', axis=1)\n",
    "df = df.drop('region_code', axis=1)\n",
    "df = df.drop('waterpoint_type', axis=1)\n",
    "df = df.drop('quantity', axis=1)\n",
    "df = df.drop('water_quality', axis=1)\n",
    "df = df.drop('payment', axis=1)\n",
    "df = df.drop('extraction_type', axis=1)\n",
    "df = df.drop('extraction_type_group', axis=1)\n",
    "df = df.drop('management', axis=1)\n",
    "df = df.drop('id', axis=1) #no need to keep it!!!\n",
    "df = df.drop('scheme_name', axis=1) ## We already have the scheme management, which is a more reasonable summary of the data. \n",
    "df = df.drop('recorded_by', axis =1) # has one value only, no discriminatory power\n",
    "df = df.drop('wpt_name', axis =1) # the name of a waterpoint should have no discriminatory value, as such we remove it\n",
    "df = df.drop('district_code', axis =1) # we have no information to what code does the district correspond, there should be 31 distinct values. However we have 20 values here. We will drop this since such flawed information may become noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 1: \n",
    "Now that we have filled missing values and deleted duplicate columns we will start working on the dataframe df_semi to further clean the remaining columns to make our data more useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_semi = df\n",
    "df_semi.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this moment we will ignore all boolean variables; public_meeting and permit. We we will start with the numerical values, to see if it makes sense transforming it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_semi.amount_tsh.value_counts() ##\n",
    "df_semi.gps_height.value_counts() ### height can have 0s and negative numbers. leave as is for now\n",
    "df_semi.longitude.value_counts() #### highly unlikely to have 0 values here. However changing this value using average location (for example of the basins) could misalocate the water point.\n",
    "df_semi.latitude.value_counts() # same as lattitude\n",
    "df_semi.num_private.value_counts()  #### no idea what it could be, no need to change\n",
    "df_semi.population.value_counts() ### it is fine to have 0s, water points can be in places with no populations\n",
    "df_semi.construction_year.value_counts()  #### 0s do not make sense, however leave as is as an alternative seems unfeasable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_semi.funder.value_counts() #### has a large amount of distinct values. We will have to engineer here a lot\n",
    "df_semi.installer.value_counts() #### same issue as funder, we will have to engineer a lot\n",
    "df_semi.basin.value_counts()  #### no need to work with this data, its perfectly fine as is.\n",
    "df_semi.subvillage.value_counts() #### 19288 different values, with no simple way to clean this.... we might have to drop this variable given the amount of geographical data we have already\n",
    "df_semi.region.value_counts() #### no need to work with this data, its perfectly fine as is.\n",
    "df_semi.lga.value_counts() #### only 125 variables, might consider dummifying it or label encode it with numerical values\n",
    "df_semi.ward.value_counts() #### 2092 variables, consider label encode it with numerical values\n",
    "df_semi.scheme_management.value_counts() #### no need to work with this data, its perfectly fine as is.\n",
    "df_semi.extraction_type_class.value_counts() #### no need to work with this data, its perfectly fine as is.\n",
    "df_semi.management_group.value_counts() #### no need to work with this data, its perfectly fine as is.\n",
    "df_semi.payment_type.value_counts() #### no need to work with this data, its perfectly fine as is.\n",
    "df_semi.quality_group.value_counts() #### no need to work with this data, its perfectly fine as is.\n",
    "df_semi.quantity_group.value_counts() #### no need to work with this data, its perfectly fine as is.\n",
    "df_semi.source_class.value_counts() #### no need to work with this data, its perfectly fine as is.\n",
    "df_semi.waterpoint_type_group.value_counts() #### no need to work with this data, its perfectly fine as is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funder: \n",
    "Has several distinct values. Therefore we will ordering them based on the overall theme. In some cases its due to typos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_semi.funder.value_counts()\n",
    "# lets begin with religious organization, we will define a list religious organizations them and replace every instance with 'Religious order'\n",
    "religious = [\"commission\", \"churc\", \"mission\", \"roman\", \"crist\", \"catho\", \"christ\", \"muslim\", \"World Vision\", \"pente\", \"Tcrs\",\n",
    "             \"islam\", \"kkkt\", \"dwe\", \"adra\", \"wvt\", \"bsf\", \"Diocese\"]\n",
    "for x in religious:\n",
    "    df_semi.loc[df_semi['funder'].str.contains(x, case=False), 'funder'] = 'Religious order'\n",
    "\n",
    "# We see several instances of '0', given  that we already have an umbrela term for unknown values. we will replace '0's for 'Unknown' \n",
    "df_semi.loc[df_semi['funder'].str.contains('0', case=False), 'funder'] = 'Unknown'\n",
    "\n",
    "    \n",
    "#There are several typos related with 'World Bank', it's their turn to be fixed\n",
    "world_bank = [\"w.b\", \"wb\", \"world bank\", \"w0rld bank\", \"world_bank\", \"World\"]\n",
    "for x in world_bank:\n",
    "    df_semi.loc[df_semi['funder'].str.contains(x, case=False), 'funder'] = 'World Bank'  \n",
    "\n",
    "# Now, to group all the United Nations agencies  funders under 'United Nations', this will also inclue the World Bank\n",
    "UN = ['Unicef', \"Unhcr\", \"undp\", \"FAO\", \"ifad\", \"imf\", \"Unesco\", \"World Bank\", \"Tasaf\", \"Unice\", \"rc\", \"unis\"]\n",
    "for x in UN:\n",
    "    df_semi.loc[df_semi['funder'].str.contains(x, case=False), 'funder'] = 'United Nations'\n",
    "    \n",
    "#For other private initiatives  \n",
    "private = ['Ikeuchi', \"private\", \"arab comm\", \"arabs\", \"bank\", \"dhv\", \"Hifab\", \"gmbh\", \"wedec\", \"WU\", \"Magadini-Makiwaru wa\",\n",
    "           \"RWE\",\"Handeni Trunk Main\"   ]\n",
    "for x in private:\n",
    "    df_semi.loc[df_semi['funder'].str.contains(x, case=False), 'funder'] = 'Other private initiatives'\n",
    "    \n",
    "#Now its the turn of regional/district/local/village level governmental organizations\n",
    "district = [\"district co\", \"council\", \"local council\", \"local\", \"dist\", \"village\", \"commu\", \"region\", \"Rudep\", \"kidep\", \"lga\",\n",
    "            \"school\" ]\n",
    "for x in district:\n",
    "    df_semi.loc[df_semi['funder'].str.contains(x, case=False), 'funder'] = 'District/local'    \n",
    "    \n",
    "# Now, we will group all values related to other countries (including typos) with the value 'Foreign aid'\n",
    "foreign_aid = [\"germany\",\"german\",\"nethe\",\"finland\",\"china\",\"belgian\", \"british\",\"italy\",\"egypt\",\"iran\", \"japan\", \n",
    "               \"european union\", \"swed\",\"korea\", \"usaid\", \"usa em\", \"u.s.a\", \"holla\", \"holand\", \"niger\", \"irish\",\n",
    "               \"swiss\",\"greec\", \"foreigne\", \"canada\", \"kuwai\", \"Nerthlands\", \"usa\", \"eu\", \"embassy\", \"norad\", \"jaica\"\n",
    "               , \"African\", \"danid\", \"Hesaw\", \"Jic\", \"Wua\", \"snv\", \"CES\"]\n",
    "for x in foreign_aid:\n",
    "    df_semi.loc[df_semi['funder'].str.contains(x, case=False), 'funder'] = 'Foreign aid'\n",
    "\n",
    "#decision bodies (ministries and other central governmental institutions) we will fit them under the existing label of \"Government Of Tanzania\"\n",
    "central = [\"water\", \"ministry\", \"Government/tcrs\", \"Wsdp\", \"central\", \"tanzania\", \"government\", \"centr\", \"gov\", \"Idara ya maji\", \"tanza\"]\n",
    "for x in central:\n",
    "    df_semi.loc[df_semi['funder'].str.contains(x, case=False), 'funder'] = 'Government Of Tanzania'\n",
    "\n",
    "# for NGOS\n",
    "NGO = [\"ngos\", \"habitat\",\"wfp\", \"wwf\", \"wfp\", \"caritas\", \"cartas\" , \"internatio\",\"red cross\", \"redcro\",\"solidarm\", \"oikos\",\n",
    "       \"founda\", \"club\", \"acord\",\"kadres\", \"karadea\", \"kdrdp\",\"kinapa\",\"mavuno\", \"drdp\", \"Oxfam\", \"Dwsp\", \"Rwssp\", \"Amref\",\n",
    "      \"lsf\", \"oxfarm\", \"Concern World Wide\", \"Adb\", \"isf\", \"shipo\", \"Plan\", \"dmdd\", \"Lvia\", \"TWESA\", \"acra\", \"Sema\", \"DW\", \"DH\" ]\n",
    "for x in NGO:\n",
    "    df_semi.loc[df_semi['funder'].str.contains(x, case=False), 'funder'] = 'NGOs'\n",
    "\n",
    "#### Given that the majority of terms remaining are very small (in fact most of the 1300s terms are unique values or statsitically insignficiant), these values will have \"Others\" for now\n",
    "final_values_funder = [\"Government Of Tanzania\", \"Foreign aid\", \"NGOs\", \"Religious order\", \"District/local\", \"United Nations\", \"Unknown\", \"Other private initiatives\" ]    \n",
    "df_semi.loc[~df_semi[\"funder\"].isin(final_values_funder), \"funder\"] = \"Other\"\n",
    "\n",
    "#df_semi[df_semi['funder'].str.contains(\"finW\", case= False)] #### we used this line to investigate specific values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 2:\n",
    "With funder completed, we will save the progress under df_funder, and continue from this point onwards using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_funder = df_semi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installer:\n",
    "Now we shall proceed to use the same transformations for installer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets begin with religious organization, we will define a list religious organizations them and replace every instance with 'Religious order'\n",
    "for x in religious:\n",
    "    df_funder.loc[df_funder['installer'].str.contains(x, case=False), 'installer'] = 'Religious order'\n",
    "\n",
    "# We see several instances of '0', given  that we already have an umbrela term for unknown values. we will replace '0's for 'Unknown' \n",
    "df_funder.loc[df_funder['installer'].str.contains('0', case=False), 'installer'] = 'Unknown'\n",
    "   \n",
    "#There are several typos related with 'World Bank', it's their turn to be fixed\n",
    "for x in world_bank:\n",
    "     df_funder.loc[df_funder['installer'].str.contains(x, case=False), 'installer'] = 'World Bank' \n",
    "        \n",
    "# Now, to group all the United Nations agencies  funders under 'United Nations', this will also inclue the World Bank\n",
    "for x in UN:\n",
    "     df_funder.loc[df_funder['installer'].str.contains(x, case=False), 'installer'] = 'United Nations'\n",
    "        \n",
    "######## For other private initiatives ##########   REMEMBER TO IMPROVE \n",
    "for x in private:\n",
    "     df_funder.loc[df_funder['installer'].str.contains(x, case=False), 'installer'] = 'Other private initiatives'\n",
    "    \n",
    "#Now its the turn of regional/district/local/village level governmental organizations\n",
    "for x in district:\n",
    "     df_funder.loc[df_funder['installer'].str.contains(x, case=False), 'installer'] = 'District/local'    \n",
    "    \n",
    "# Now, we will group all values related to other countries (including typos) with the value 'Foreign aid'\n",
    "for x in foreign_aid:\n",
    "     df_funder.loc[df_funder['installer'].str.contains(x, case=False), 'installer'] = 'Foreign aid'\n",
    "\n",
    "    \n",
    "#For all decision bodies (ministries and other central governmental institutions) we will fit them under the existing label of \"Government Of Tanzania\"\n",
    "for x in central:\n",
    "     df_funder.loc[df_funder['installer'].str.contains(x, case=False), 'installer'] = 'Government Of Tanzania'\n",
    "\n",
    "# for NGOS\n",
    "for x in NGO:\n",
    "     df_funder.loc[df_funder['installer'].str.contains(x, case=False), 'installer'] = 'NGOs'\n",
    "        \n",
    "#### same as before, the remaining values will have \"Others\" for now\n",
    "final_values_installer = [\"Government Of Tanzania\", \"Foreign aid\", \"NGOs\", \"Religious order\", \"District/local\", \"United Nations\", \"Unknown\", \"Other private initiatives\" ]    \n",
    "df_funder.loc[~df_funder[\"installer\"].isin(final_values_installer), \"installer\"] = \"Other\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Checkpoint 3:\n",
    "With funder completed, we will save the progress under df_installer , and continue from this point onwards using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_installer = df_funder #### with installer done, we will utilize df_installer for the next transformation\n",
    "df_installer = df_installer.drop('subvillage', axis=1) ### we have decided to drop this last variable given that we already have sufficient geographical data, and due to the considerable size of distinct values  (over 19k and no simple way to sumamrise them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_finalv1 = df_installer \n",
    "df_finalv1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have finished with the training set cleaning, now lets proceed with the testing set cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftest = pd.read_csv('Test set values.csv', header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids = pd.DataFrame(dftest['id'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Na's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftest['funder'] =dftest.funder.fillna(\"Unknown\")\n",
    "dftest['installer'] =dftest.installer.fillna(\"Unknown\")\n",
    "dftest['subvillage'] =dftest.subvillage.fillna(\"Unknown\")\n",
    "dftest['scheme_management'] =dftest.scheme_management.fillna(\"Unknown\")\n",
    "dftest['scheme_name'] =dftest.scheme_name.fillna(\"Unknown\")\n",
    "dftest['public_meeting'] =df.public_meeting.fillna(False)\n",
    "dftest['permit'] =df.permit.fillna(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftest = dftest.drop('source', axis=1)\n",
    "dftest = dftest.drop('source_type', axis=1)\n",
    "dftest = dftest.drop('region_code', axis=1)\n",
    "dftest = dftest.drop('waterpoint_type', axis=1)\n",
    "dftest = dftest.drop('quantity', axis=1)\n",
    "dftest = dftest.drop('water_quality', axis=1)\n",
    "dftest = dftest.drop('payment', axis=1)\n",
    "dftest = dftest.drop('extraction_type', axis=1)\n",
    "dftest = dftest.drop('extraction_type_group', axis=1)\n",
    "dftest = dftest.drop('management', axis=1)\n",
    "dftest = dftest.drop('id', axis=1)\n",
    "dftest = dftest.drop('scheme_name', axis=1)  \n",
    "dftest = dftest.drop('recorded_by', axis =1) \n",
    "dftest = dftest.drop('wpt_name', axis =1) \n",
    "dftest = dftest.drop('district_code', axis =1) \n",
    "dftest = dftest.drop('subvillage', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for religions\n",
    "for x in religious:\n",
    "    dftest.loc[dftest['funder'].str.contains(x, case=False), 'funder'] = 'Religious order'\n",
    "\n",
    "# 0s added to unknown\n",
    "dftest.loc[dftest['funder'].str.contains('0', case=False), 'funder'] = 'Unknown'\n",
    "    \n",
    "#For World Bank\n",
    "for x in world_bank:\n",
    "    dftest.loc[dftest['funder'].str.contains(x, case=False), 'funder'] = 'World Bank'  \n",
    "\n",
    "# For the UN\n",
    "for x in UN:\n",
    "    dftest.loc[dftest['funder'].str.contains(x, case=False), 'funder'] = 'United Nations'    \n",
    "    \n",
    "#For other private initiatives  \n",
    "for x in private:\n",
    "    dftest.loc[dftest['funder'].str.contains(x, case=False), 'funder'] = 'Other private initiatives'\n",
    "    \n",
    "#For regional/district/local/village level governmental organizations\n",
    "for x in district:\n",
    "    dftest.loc[dftest['funder'].str.contains(x, case=False), 'funder'] = 'District/local'    \n",
    "    \n",
    "# For 'Foreign aid'\n",
    "for x in foreign_aid:\n",
    "    dftest.loc[dftest['funder'].str.contains(x, case=False), 'funder'] = 'Foreign aid'\n",
    "    \n",
    "#For central government\n",
    "for x in central:\n",
    "    dftest.loc[dftest['funder'].str.contains(x, case=False), 'funder'] = 'Government Of Tanzania'\n",
    "\n",
    "# for NGOS\n",
    "for x in NGO:\n",
    "    dftest.loc[dftest['funder'].str.contains(x, case=False), 'funder'] = 'NGOs'\n",
    "# for the rest\n",
    "dftest.loc[~dftest[\"funder\"].isin(final_values_funder), \"funder\"] = \"Other\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x in religious:\n",
    "    dftest.loc[dftest['installer'].str.contains(x, case=False), 'installer'] = 'Religious order'\n",
    "\n",
    "# 0s added to unknown\n",
    "dftest.loc[dftest['installer'].str.contains('0', case=False), 'installer'] = 'Unknown'\n",
    "    \n",
    "#TFor World Bank\n",
    "for x in world_bank:\n",
    "    dftest.loc[dftest['installer'].str.contains(x, case=False), 'installer'] = 'World Bank'  \n",
    "\n",
    "# For the UN\n",
    "for x in UN:\n",
    "    dftest.loc[dftest['installer'].str.contains(x, case=False), 'installer'] = 'United Nations'\n",
    "    \n",
    "#For other private initiatives  \n",
    "for x in private:\n",
    "    dftest.loc[dftest['installer'].str.contains(x, case=False), 'installer'] = 'Other private initiatives'\n",
    "    \n",
    "#For regional/district/local/village level governmental organizations\n",
    "for x in district:\n",
    "    dftest.loc[dftest['installer'].str.contains(x, case=False), 'installer'] = 'District/local'    \n",
    "    \n",
    "# For 'Foreign aid'\n",
    "for x in foreign_aid:\n",
    "    dftest.loc[dftest['installer'].str.contains(x, case=False), 'installer'] = 'Foreign aid'\n",
    "    \n",
    "#For central government\n",
    "for x in central:\n",
    "    dftest.loc[dftest['installer'].str.contains(x, case=False), 'installer'] = 'Government Of Tanzania'\n",
    "\n",
    "# for NGOS\n",
    "for x in NGO:\n",
    "    dftest.loc[dftest['installer'].str.contains(x, case=False), 'installer'] = 'NGOs'\n",
    "# for the rest\n",
    "dftest.loc[~dftest[\"installer\"].isin(final_values_installer), \"installer\"] = \"Other\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dftest1 = dftest\n",
    "dftest1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date_recorded: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the variable is a string. This is not useful, therefor we will separate this variable into 3 new variables 'year', 'month' and 'day' (which will be numericals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_finalv1 = pd.concat([df_finalv1.drop('date_recorded', axis = 1), \n",
    "          (df_finalv1.date_recorded.str.split(\"-|T\").str[:3].apply(pd.Series)\n",
    "          .rename(columns={0:'year', 1:'month', 2:'day'}))], axis = 1)\n",
    "\n",
    "dftest1 =  pd.concat([dftest1.drop('date_recorded', axis = 1), \n",
    "          (dftest1.date_recorded.str.split(\"-|T\").str[:3].apply(pd.Series)\n",
    "          .rename(columns={0:'year', 1:'month', 2:'day'}))], axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_finalv1.year = df_finalv1.year.astype(float)\n",
    "df_finalv1.month = df_finalv1.month.astype(float)\n",
    "df_finalv1.day = df_finalv1.day.astype(float)\n",
    "\n",
    "dftest1.year = dftest1.year.astype(float)\n",
    "dftest1.month = dftest1.month.astype(float)\n",
    "dftest1.day = dftest1.day.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model\n",
    "For this first model, we shall use a quick and rough label encoder (using numerical values for both the training and the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Quick and rough label encoder (using the # of classes per variables as the numerical values). We are very much aware that this way has several limiations, as it may give added weight to values with little importance. Nevertheless this for a base model this is perfectly fine as we will improve this model over the next iterations \n",
    "for f in df_finalv1.columns:\n",
    "    if df_finalv1[f].dtype=='object':\n",
    "        print(f)\n",
    "        lbl=preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(df_finalv1[f].values))\n",
    "        df_finalv1[f]=lbl.transform(list(df_finalv1[f].values))\n",
    "        \n",
    "for f in dftest1.columns:\n",
    "    if dftest1[f].dtype=='object':\n",
    "        print(f)\n",
    "        lbl=preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(dftest1[f].values))\n",
    "        dftest1[f]=lbl.transform(list(dftest1[f].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see the result of our label encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_finalv1.head()\n",
    "dftest1. head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We turn our dataframes into numpy arrays, given that sickit learn is better optimised for this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions1 = predictions.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 1: Decision Tree Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DTC = DecisionTreeClassifier(max_depth=None, min_samples_split=10,random_state=55)\n",
    "#scores1 = cross_val_score(DTC, df_finalv1, predictions1.ravel(), cv=5)\n",
    "#scores1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 2: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RFC = RandomForestClassifier(bootstrap=True, n_estimators=700, max_depth=None, min_samples_split=3, max_features='auto',\n",
    "                             class_weight=None  ,min_samples_leaf= 1, random_state=0, n_jobs = -1)\n",
    "#scores2 = cross_val_score(RFC, df_finalv1, predictions1.ravel(), cv=5)\n",
    "#scores2.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 3: Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ETC =  ExtraTreesClassifier(bootstrap=True, n_estimators=500, max_depth=None, min_samples_split=2, random_state=0)\n",
    "#scores3 = cross_val_score(ETC, df_finalv1, predictions1.ravel(), cv=5)\n",
    "#scores3.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We determine that out of the 3 models, Random forest classifier yields the best results. Hence we will use gridsearch to find the best parameters for our model and at the same time implement cross validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [650, 690 ],\n",
    "    'max_features': ['auto'], \n",
    "    'max_depth': [21]\n",
    "    \n",
    "}\n",
    "\n",
    "model = GridSearchCV(RFC, param_grid, cv=5)\n",
    "model.fit(df_finalv1, predictions1.ravel()) # the ravel() here was used to transform prediction into a flattened array (a warnign was given if we kept it as a vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that this part took trial an error, the parameters above are the results of the tinckering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(df_finalv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see num_private, public_meeting, permit have a very low importance. We will remove them and test again (however we will keep 'year' in spite of the low importance as the effect with 'month' and 'day' is well worth it). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_finalv1 = df_finalv1.drop('num_private', axis=1)\n",
    "df_finalv1 = df_finalv1.drop('public_meeting', axis=1)\n",
    "df_finalv1 = df_finalv1.drop('permit', axis=1)\n",
    "\n",
    "dftest1 = dftest1.drop('num_private', axis=1)\n",
    "dftest1 = dftest1.drop('public_meeting', axis=1)\n",
    "dftest1 = dftest1.drop('permit', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [685, 690, 705 ],\n",
    "    'max_features': ['auto'], \n",
    "    'max_depth': [21, 22]\n",
    "    \n",
    "}\n",
    "model = GridSearchCV(RFC, param_grid, n_jobs=8, cv=5)\n",
    "model.fit(df_finalv1, predictions1.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(df_finalv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional removal of variables based on importance gave worse values. Therefore we will conclude our model here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to save and submit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = model.predict(dftest1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results =  pd.DataFrame(results, columns=['status_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = pd.concat([ids, results], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results.to_csv('prediction33.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model yielded 0.8211 score (position #255 out of #4638) in the competition."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
